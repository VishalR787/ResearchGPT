[
  {
    "title": "The Artificial Scientist: Logicist, Emergentist, and Universalist Approaches to Artificial General Intelligence",
    "authors": [
      "Michael Timothy Bennett",
      "Yoshihiro Maruyama"
    ],
    "year": 2021,
    "abstract": "We attempt to define what is necessary to construct an Artificial Scientist, explore and evaluate several approaches to artificial general intelligence (AGI) which may facilitate this, conclude that a unified or hybrid approach is necessary and explore two theories that satisfy this requirement to some degree.",
    "raw_text": "arXiv:2110.01831v1  [cs.AI]  5 Oct 2021\nThe Arti\ufb01cial Scientist:\nLogicist, Emergentist, and Universalist\nApproaches to Arti\ufb01cial General Intelligence\u22c6\nMichael Timothy Bennett and Yoshihiro Maruyama\nSchool of Computing, Australian National University, Canberra, Australia\nmichael.bennett@anu.edu.au and yoshihiro.maruyama@anu.edu.au\nAbstract. We attempt to de\ufb01ne what is necessary to construct an Arti-\n\ufb01cial Scientist, explore and evaluate several approaches to arti\ufb01cial gen-\neral intelligence (AGI) which may facilitate this, conclude that a uni\ufb01ed\nor hybrid approach is necessary and explore two theories that satisfy this\nrequirement to some degree.\nKeywords: AGI \u00b7 AI for Science \u00b7 Science Robotics.\n1\nIntroduction\nAmong the proposed means of verifying AGI, Goertzel\u2019s 2014 survey [1] listed\nThe Arti\ufb01cial Scientist Test [2], which stipulated that AGI will have been achieved\nwhen an arti\ufb01cial intelligence (AI) independently produces research su\ufb03cient to\nwin a Nobel prize. While there is a wealth of research on AGI in general, and\nthe automation of science has been explored to some extent [28], what would\nbe required to satisfy this test remains unclear. This paper attempts to clar-\nify what exactly is necessary to create an Arti\ufb01cial Scientist, and how this \ufb01ts\nwithin existing approaches to AGI.\nA scientist may be many things, but for our purposes a simple and unam-\nbiguous de\ufb01nition is best. The Royal Society\u2019s motto, \u201cnullius in verba\u201d, serves\nnicely. Translated as \u201ctake nobody\u2019s word for it\u201d, it emphasises that a scientist\nestablishes truth through experiment, not testimony. For this, our agent must\npossess certain qualities.\n2\nWhat is Required of an Arti\ufb01cial Scientist?\nThis is not a list of every quality an Arti\ufb01cial Scientist ought to posses, but an\nattempt to identify what is necessary.\nRepresentation of Hypotheses: We\u2019ll de\ufb01ne a hypothesis as a statement\nwhich has a truth value. A subset of such statements are readily testable, suit-\nable subjects of scienti\ufb01c enquiry. We\u2019ll not concern ourselves with the speci\ufb01c\nlanguage used to represent a hypothesis beyond stating that an Arti\ufb01cial Scien-\ntist must possess a means of representing any particular hypothesis.\n\u22c6This work was supported by JST (JPMJMS2033; JPMJPR17G9).\n2\nM. T. Bennett and Y. Maruyama\nInductive Inference: The Royal Society\u2019s motto is an explicit rejection of\ntestimony as the basis of any claim. An Arti\ufb01cial Scientist must not rely on tes-\ntimony. Without testimony, what is true must be inferred through observation,\nand so the ability to perform inductive inference seems necessary.\nDeductive and Abductive Reasoning: Having inferred something of what is\nan agent may transform this information, without speculation, through deductive\nreasoning. Then, from what is, our agent could abduct all that may be true, but\nuncertain. A testable hypothesis is one such thing, abducted from what is known.\nIt seems necessary then for our agent to engage in deductive and abductive\nreasoning.\nCausal Reasoning and Explainability: The purpose of an experiment is to\ntest a hypothesis, identifying cause and e\ufb00ect [20]. Arguably this is desired in\norder that humans may develop the technology to reproduce or prevent that\ne\ufb00ect at will. A provisionally accepted hypothesis explains phenomena. An ex-\nplanation is only useful if it can be understood by its intended audience, and so\na scientist must be able to communicate their hypotheses and the signi\ufb01cance\nof their results in terms of what its audience values and understands. Mere in-\nterpretability is insu\ufb03cient for more complex phenomena, as interpreting even\nsimple symbolic models of well understood subject matter requires a great deal\nof technical expertise. Pushing forward the boundaries of scienti\ufb01c achievement\nwould produce models of such complexity as to be beyond the capabilities of\nhuman interpretation. Fluency in natural language is desirable.\nEvaluation of Hypotheses: A hypothesis must at least be falsi\ufb01able, positing\ncause and e\ufb00ect. If we assume computational resources are \ufb01nite, then there\nis a cost to consider in the search for hypotheses. The question then is which\nhypotheses ought to be abducted. Hume\u2019s Guillotine tells us one cannot derive\nan ought from an is, and so we must give our agent an ought by which to judge\nhypotheses. If one is to choose between several hypotheses, the truth of any one\nof which would serve to explain observed phenomena, then it seems reasonable\nto assert that one should start by testing the most plausible, the most likely to\nbe true. One must also consider what is gained by proving or disproving any\nhypothesis. Yes, one may choose to investigate with scienti\ufb01c rigour problems of\nno interest to anyone, but we would hesitate to claim this is accepted practice\nfor contemporary scientists. Hence we assert that an Arti\ufb01cial Scientist must\nhave a means of judging the plausibility of, and potential pro\ufb01t in any line of\ninquiry; a heuristic to inform its search of the space of possible hypotheses.\nExperimental Design, Evaluation and Planning: To test a hypothesis one\nmust design an experiment that isolates and tests the hypothesised cause of\nan e\ufb00ect, ideally controlling for all other variables. Each experiment costs re-\nsources, and the information gained should be evaluated in terms of expected\nbene\ufb01t across hypotheses and future experiments. For example, a valuable ex-\nperiment may not entirely con\ufb01rm or disprove any one hypothesis, but may\nThe Arti\ufb01cial Scientist\n3\nprovide information allowing an agent to more e\ufb03ciently select future experi-\nments that will con\ufb01rm or disprove many high priority hypotheses. There are also\nrisks to consider in an experiment. An experiment with a high expected utility\nmay threaten the agent\u2019s continued existence, and so some form of risk aversion\nmay be necessary (for example, when planning future experiments the geomet-\nric mean may be more appropriate than the arithmetic mean when computing\nutility, because the utility of future experiments depends upon the outcome of\npreceding experiments and their impact on available resources and capability).\nAn agent must identify what novel information would con\ufb01rm or disprove those\nabducted hypotheses of the greatest expected utility. It must design experiments\nthat will convey said novel information and compare and plan experiments based\non opportunity cost and risk.\nEnactivism: To perform experiments, an agent must possess a means of inter-\nacting with the environment. The process of experimentation could be perceived\nas enactive cognition [25], which posits cognition arises through the interaction\nof an organism with its environment. It assumes cognition is embodied, embed-\nded to function within the con\ufb01nes of a speci\ufb01c environment, enacted through\nwhat an organism does and, \ufb01nally, extending into that environment to store\nand retrieve information. All of this seems obviously necessary to conduct ex-\nperiments in the environment. We are not o\ufb00ering an unquali\ufb01ed endorsement\nof embodied cognition; after all it is arguable that even a laptop has a body\n[16]. However, experimentation has certain physical requirements, and if one is\nable to perform targeted experiments that obtain speci\ufb01c novel information and\nisolate causal relations, the process of learning may proceed much faster than if\none is forced to wait until that same novel information is observed by chance.\n3\nThree Relevant Approaches to AGI\nFor the following we draw heavily upon Goertzel\u2019s 2014 survey of the \ufb01eld [1], de-\nviating slightly to include recent developments and adjust the categories to suit\nour purposes. To standardise the terms with which we compare these approaches\nwe employ a model of an arbitrary task, we treat the application of intelligence\nas prediction, and so de\ufb01ne each of these approaches as trying to predict the ap-\npropriate response r given a situation s. For the sake of brevity appropriateness,\nsituation and response can be read using their common language de\ufb01niti"
  },
  {
    "title": "Compression, The Fermi Paradox and Artificial Super-Intelligence",
    "authors": [
      "Michael Timothy Bennett"
    ],
    "year": 2021,
    "abstract": "The following briefly discusses possible difficulties in communication with and control of an AGI (artificial general intelligence), building upon an explanation of The Fermi Paradox and preceding work on symbol emergence and artificial general intelligence. The latter suggests that to infer what someone means, an agent constructs a rationale for the observed behaviour of others. Communication then requires two agents labour under similar compulsions and have similar experiences (construct similar solutions to similar tasks). Any non-human intelligence may construct solutions such that any rationale for their behaviour (and thus the meaning of their signals) is outside the scope of what a human is inclined to notice or comprehend. Further, the more compressed a signal, the closer it will appear to random noise. Another intelligence may possess the ability to compress information to the extent that, to us, their signals would appear indistinguishable from noise (an explanation for The Fermi Paradox). To facilitate predictive accuracy an AGI would tend to more compressed representations of the world, making any rationale for their behaviour more difficult to comprehend for the same reason. Communication with and control of an AGI may subsequently necessitate not only human-like compulsions and experiences, but imposed cognitive impairment.",
    "raw_text": "arXiv:2110.01835v1  [cs.AI]  5 Oct 2021\nCompression, The Fermi Paradox\nand Arti\ufb01cial Super-Intelligence\nMichael Timothy Bennett\nSchool of Computing, Australian National University, Canberra, Australia\nmichael.bennett@anu.edu.au\nAbstract. The following brie\ufb02y discusses possible di\ufb03culties in com-\nmunication with and control of an AGI (arti\ufb01cial general intelligence),\nbuilding upon an explanation of The Fermi Paradox and preceding work\non symbol emergence and arti\ufb01cial general intelligence. The latter sug-\ngests that to infer what someone means, an agent constructs a rationale\nfor the observed behaviour of others. Communication then requires two\nagents labour under similar compulsions and have similar experiences\n(construct similar solutions to similar tasks). Any non-human intelligence\nmay construct solutions such that any rationale for their behaviour (and\nthus the meaning of their signals) is outside the scope of what a hu-\nman is inclined to notice or comprehend. Further, the more compressed\na signal, the closer it will appear to random noise. Another intelligence\nmay possess the ability to compress information to the extent that, to\nus, their signals would appear indistinguishable from noise (an explana-\ntion for The Fermi Paradox). To facilitate predictive accuracy an AGI\nwould tend to more compressed representations of the world, making\nany rationale for their behaviour more di\ufb03cult to comprehend for the\nsame reason. Communication with and control of an AGI may subse-\nquently necessitate not only human-like compulsions and experiences,\nbut imposed cognitive impairment.\nKeywords: compression \u00b7 symbol emergence \u00b7 communication.\n1\nIntroduction\nWhen examining what problems may arise in the pursuit of AGI, it may be-\nhoove us to consider explanations for The Fermi Paradox [18], the contradiction\nbetween the apparent absence of extra-terrestrial life and its high probability.\nAfter all, both involve communication with a nonhuman intelligence.\nSo, let us assume for the sake of argument that a non-human intelligence\nexists in our region of space, emitting signals in a similar medium to us (such as\nradio) neither attempting to contact nor hide from us; why might we have failed\nto identify or interpret the meaning of such signals, and what does this suggest\nfor the pursuit of AGI?\n2\nM. T. Bennett\n2\nSymbolic Abstraction\nFirst, let us consider what is necessary to infer the meaning of something. Natural\nlanguage is a means of encoding and transmitting certain information between\nmembers of a species. What this information is, is debatable. A natural language\nmodel such as GPT-3 is trained only on text data [2], implicitly endorsing the\nidea that meaning is just relations between words. While GPT-3 is capable of\nlearning correlations in syntax to the extent that it can plausibly mimic human\nwriting, it lacks any of the other sensorimotor information we might typically\nassociate with words. Attempts to train models on multimodal sensorimotor data\nhave yielded some success, with agents able to associate the sensory information\nof an object such as a cup with the signals that represent it [7,8,9]. Yet abstract\nnotions such as \u201cpolitics\u201d or \u201cex-wife\u201d would seem to require more than mere\nclustering of sensorimotor information.\nOne theory [10] (the mirror symbol hypothesis), posits that the informa-\ntion encoded in natural language is not just sensorimotor stimuli but intent.\nDrawing on ideas from embodied and enactive cognition [17,16], an organism\u2019s\nenvironment, sensors and actuators, the compulsions an organism labours under\n(such as hunger and pain) and so forth together specify an arbitrary task. Util-\nity is replaced by a statement (a logical expression) characterizing sets of more\nor less desirable sensorimotor states (including the state of memory) - repre-\nsenting histories or situations from which plans and subsequently subgoals may\nbe abducted. It is written in a physically implementable language such as ar-\nrangements of transistors or neurons, necessary and su\ufb03cient to reconstruct past\nexperience given appropriate stimuli. If treated as a constraint to be satis\ufb01ed,\na solution or any subgoal derived thereof, expresses intent. Given an ostensive\nde\ufb01nition of a task (examples of successful task completion) there may be many\napparently valid solutions, which vary in how well they generalise to unfore-\nseen situations. The weaker and more general the solution, the closer it is to an\nidealised notion of intent (called an intensional solution).\nIn order to predict the intent of other agents, one agent assumes others con-\nceive of the world as they do. It asks what subgoals might motivate the behaviour\nof other agents, given they are assumed to pursue a similar solution in general.\nIt constructs a rationale for speci\ufb01c observed behaviour, to explain what an-\nother agent means to do (a subgoal, the pursuit of which would explain the\nobserved behaviour). Subsequently, in order for communication to be possible\ntwo agents must possess approximately the same solution. Not only must they\nexperience similar stimuli with which to construct symbolic abstractions, but\nimbue that stimuli with similar signi\ufb01cance in terms of satisfying their com-\npulsions. The solutions they construct then facilitate encoding and decoding of\nsignals interpretable by both agents [11]. Any information not relevant to satis-\nfying compulsions is not only meaningless but may be entirely ignored, which is\nconsistent with observations of human behaviour [6,5] (i.e. one may be unable\nto perceive something in the stream of sensorimotor stimuli because it is \ufb01ltered\nout).\nCompression, The Fermi Paradox and Arti\ufb01cial Super-Intelligence\n3\nThis raises a few issues. The scope of a task is arbitrary, and so living as\na typical human may be framed as such. Any nonhuman intelligence may face\nan entirely di\ufb00erent task, to which they construct an entirely di\ufb00erent solution.\nThe di\ufb03culty this introduces is not only failure to understand what is meant,\nas in human language translation. Given two di\ufb00erent solutions to two di\ufb00erent\ntasks, stimuli may be imbued with either the same meaning, di\ufb00erent meaning,\nor no meaning at all by one of those solutions (meaningful to one but not the\nother). The latter is particularly interesting, because such signals may not be\nrecognised as intelligent behaviour (i.e. appear insane or mindless) or may per-\nhaps be unnoticeable (i.e. the solution informs attention). In short, we may not\nrealise something is a signal because what it conveys falls outside the scope of\nwhat humans are predisposed to notice or consider meaningful. While by de\ufb01-\nnition we would be disinterested in such information, it may imply something\nwhich we would be consider meaningful (e.g. destroying the humans because of\nincomprehensible reasons).\n3\nCompression\nAllowing for the above, one may still mechanically assess information content\nand decode signals [1], to glean something of potential meanings by correlation,\neven if they are incomprehensible. However, the same information can be repre-\nsented in many di\ufb00erent ways, compressed to di\ufb00erent extents. As the volume of\ndigital information exchanged and stored by humans each day has increased, so\nhas the utility of compression. Streaming services such as Youtube make exten-\nsive use of compression to reduce the cost of transmission and storage. Another\nintelligence may also wish to reduce the cost of transmission and storage by\nemploying the most e\ufb00ective compression they possess. Taking into account the\ndecoder which reconstructs a signal, the greatest extent to which a signal may\nbe compressed is its Kolmogorov Complexity [12]; the length of the smallest self\nextracting archive capable of reproducing that signal. Such a compressed sig-\nnal contains no discernible pattern of which we might take advantage to more\ne\ufb03ciently represent the signal without discarding information. Uniformly dis-\ntributed, random noise is also not compressible. There is no "
  },
  {
    "title": "Creative Problem Solving in Artificially Intelligent Agents: A Survey and Framework",
    "authors": [
      "Evana Gizzi",
      "Lakshmi Nair",
      "Sonia Chernova",
      "Jivko Sinapov"
    ],
    "year": 2022,
    "abstract": "Creative Problem Solving (CPS) is a sub-area within Artificial Intelligence (AI) that focuses on methods for solving off-nominal, or anomalous problems in autonomous systems. Despite many advancements in planning and learning, resolving novel problems or adapting existing knowledge to a new context, especially in cases where the environment may change in unpredictable ways post deployment, remains a limiting factor in the safe and useful integration of intelligent systems. The emergence of increasingly autonomous systems dictates the necessity for AI agents to deal with environmental uncertainty through creativity. To stimulate further research in CPS, we present a definition and a framework of CPS, which we adopt to categorize existing AI methods in this field. Our framework consists of four main components of a CPS problem, namely, 1) problem formulation, 2) knowledge representation, 3) method of knowledge manipulation, and 4) method of evaluation. We conclude our survey with open research questions, and suggested directions for the future.",
    "raw_text": "Creative Problem Solving in Intelligent Agents: A Framework and Survey\nCreative Problem Solving in Arti\ufb01cially Intelligent Agents:\nA Survey and Framework\nEvana Gizzi\u2217\nevana.gizzi@tufts.edu\nDepartment of Computer Science, Tufts University,\nMedford, MA 02155\nLakshmi Nair\u2217\nlnair3@gatech.edu\nCollege of Computing, Georgia Tech,\nAtlanta, GA 30332\nSonia Chernova\nchernova@gatech.edu\nCollege of Computing, Georgia Tech,\nAtlanta, GA 30332\nJivko Sinapov\njivko.sinapov@tufts.edu\nDepartment of Computer Science, Tufts University,\nMedford, MA 02155\nAbstract\nCreative Problem Solving (CPS) is a sub-area within Arti\ufb01cial Intelligence (AI) that\nfocuses on methods for solving o\ufb00-nominal, or anomalous problems in autonomous systems.\nDespite many advancements in planning and learning, resolving novel problems or adapting\nexisting knowledge to a new context, especially in cases where the environment may\nchange in unpredictable ways post deployment, remains a limiting factor in the safe and\nuseful integration of intelligent systems. The emergence of increasingly autonomous systems\ndictates the necessity for AI agents to deal with environmental uncertainty through creativity.\nTo stimulate further research in CPS, we present a de\ufb01nition and a framework of CPS,\nwhich we adopt to categorize existing AI methods in this \ufb01eld. Our framework consists of\nfour main components of a CPS problem, namely, 1) problem formulation, 2) knowledge\nrepresentation, 3) method of knowledge manipulation, and 4) method of evaluation. We\nconclude our survey with open research questions, and suggested directions for the future.\n1. Introduction\nCreativity is often described as a hallmark of sophisticated intelligence. The Oxford English\nDictionary de\ufb01nes \u201ccreativity\u201d as \u201cInventive, imaginative; of, relating to, displaying, using,\nor involving imagination or original ideas as well as routine skill or intellect, esp.\nin\nliterature or art\u201d (Dictionary, 1989). Despite our familiarity with the notion of creativity,\nunderstanding and implementing creativity in arti\ufb01cially intelligent systems continues to\nbe a challenge. Computational Creativity (CC) is an active area of research that seeks to\ndevelop computational methods that are capable of generating a creative output, reminiscent\nof the creative processes in humans. The CC research community includes a diverse body of\nresearchers, spanning the \ufb01elds of psychology, neuroscience, philosophy, and computer science.\n\u2217. Both authors contributed equally\n1\narXiv:2204.10358v1  [cs.AI]  21 Apr 2022\nGizzi, Nair, Sinapov & Chernova\nFigure 1: Examples of CPS in human and non-human species: The jury-rigged \ufb01lter\nconstructed by the astronauts on Apollo 13 (left, image credit: NASA). Rook extracting a\nbucket by bending a piece of wire to make a hook (right) (Bird & Emery, 2009).\nThe goal of CC research, as described by the Association for Computational Creativity1, is\n\u201c[to gain] the ability to model, simulate or replicate creativity using a computer, to achieve one\nof several ends, including the construction of a program or computer capable of human-level\ncreativity, to better understand human creativity and to formulate an algorithmic perspective\non creative behavior in humans, and to design programs that can enhance human creativity\nwithout necessarily being creative themselves\u201d (ICCC, ). While there has been extensive\nwork in the area of Computational Creativity in Arti\ufb01cial Intelligence (AI), these works\nare primarily focused on the generation of creative artifacts, e.g., paintings, poems etc.\nIn contrast, there is very limited focus on creativity that is speci\ufb01cally task-oriented, i.e.,\ncreativity in problem solving.\nCreative problem solving (CPS) focuses on using creative processes in the context of\nproblem solving. Both human and non-human species have been shown to creatively solve\nproblems (Boesch & Boesch, 1990; Baker, Kanitscheider, Markov, Wu, Powell, McGrew, &\nMordatch, 2019) (Figure 1), e.g., crows have been shown to spontaneously modify tools by\nshaping hooks out of wire, and using the modi\ufb01ed tools in the correct sequence of actions\nrequired to retrieve food (Bird & Emery, 2009). Various models of CPS in humans have\nalso been proposed by researchers over the years (Isaksen & Tre\ufb03nger, 2004), beginning as\nearly as 1952 with the work of Alex Osborn, who presented a comprehensive description\nof a seven-stage CPS process (Osborn, 1952). Prior work by Mumford et al. (Mumford,\nMobley, Reiter-Palmon, Uhlman, & Doares, 1991; Mumford, Supinski, Baughman, Costanza,\n& Threlfall, 1997) has also investigated CPS processes in humans by conducting human\nparticipant studies and evaluations. They de\ufb01ne creative problems as problems that arise in\nill-de\ufb01ned situations, thus eliciting creativity in humans. Further, their work highlights core\ncognitive processes involved in CPS, including problem construction, information encoding,\nidea evaluation, implementation, and monitoring. While these works primarily explore CPS\nin humans and other animals, there is very limited work focusing on CPS in arti\ufb01cial agents.\nWhat makes CPS important for AI? Numerous real-world examples demonstrate\nthe practical importance of CPS, particularly when dealing with crises or time-constrained\nscenarios. For instance, in the Apollo 13 incident of 1970, astronauts on board the spacecraft\n1. The Association of Computational Creativity is a nonpro\ufb01t organization dedicated to the advancement of\nCC, and organizing body of the International Conference on Computational Creativity (ICCC)\n2\nCreative Problem Solving in Intelligent Agents: A Framework and Survey\ncreatively constructed a makeshift CO2 \ufb01lter using unconventional materials, enabling them\nto safely return home (Cass, 2005) (See left, Figure 1). More recently, makeshift ventilators\nbuilt using low-cost 3D printed parts and o\ufb00-the-shelf items such as manual resuscitator\nPVC bags and motors, have been used to combat the widespread equipment shortages\nduring COVID-19 (Kitchen, 2020; Turner, Duggan, Glezerson, & Marshall, 2020). However,\nsimilar skills are currently beyond the scope of AI. Developing arti\ufb01cial agents with similar\ncapabilities, can greatly improve the resourcefulness and adaptability of existing AI systems.\nThese capabilities will be especially useful for robots that explore, as well as work in space,\nunderwater, remote locations on land, and disaster sites, where the robots are highly likely\nto face unprecedented circumstances, requiring them to adapt. (Atkeson, Benzun, Banerjee,\nBerenson, Bove, Cui, DeDonato, Du, Feng, Franklin, et al., 2018).\nIn this survey, we describe how CPS can leverage concepts from CC and planning/learning\nin AI to improve the adaptability of existing AI systems to novel scenarios. Similar surveys\nhave previously focused speci\ufb01cally on CC by reviewing interdisciplinary work in CC along\nwith evaluation techniques for CC systems ((Jordanous, 2013), (Lamb, Brown, & Clarke,\n2018)). Rowe et al. (Rowe & Partridge, 1993) surveyed CC works explicitly in an AI\ncontext, in which they suggest \ufb01ve key aspects of CC systems. These include a) \ufb02exibility\nof knowledge representations, b) tolerance to ambiguity in the knowledge representations,\nc) avoiding functional \ufb01xity, d) assessing the usefulness of the creative output, and e) the\ncapacity to elaborate on the creative output to \ufb01nd out their consequences. The special issue\njournal on \u201cProblem-solving, Creativity and Spatial Reasoning\u201d by Falomir et al. (Falomir &\nOlte\u0163eanu, 2019) compiled selected works in existing CPS research for a multi-disciplinary\nperspective on problem solving in CC, focused speci\ufb01cally on highlighting the synergies\nbetween the traditionally separate research areas. In contrast to prior surveys, to the best of\nour knowledge, this is the \ufb01rst survey that is speci\ufb01cally focused on creative problem solving\nin AI, leveraging the literature from both CC and AI. Our goal in this survey is to contribute\na taxonomy of research in CPS, and to provide organ"
  }
]